# AAI_520_FinalProjectTeam6
   This project deals with creating a working chatbot trained on Stanford question answering data set (squad 2.0). Chatbots have become integral part of our day-to-day life and in almost all industries. So, it is essential that AI Engineers know all the nuances of its creation, training and maintenance. Squad 2.0 data set contains large corpus of data collected on various topics from Music to Physics.

The squad 2.0 data set is downloaded from Kaggle (https://www.kaggle.com/datasets/stanford) as the base to train our models to be used in the chatbot. The data was in Json format with a field called “context” and a set of questions based on context.

To explore various transformer-based techniques, the chatbot was developed using three distinct approaches:

1. Utilizing the Huggingface Transformers pipeline with state-of-the-art models like Roberta and Llama3.1.
2. Employing the advanced capabilities of GPT-3 to generate contextually relevant answers.
3. Implementing the Text-to-Text Transfer Transformer (T5) model, which has been fine-tuned on the SQuAD dataset and enhanced with a context retrieval mechanism to improve response accuracy.
   
Each approach offers unique insights into the practical applications of transformer technologies in real-world AI tasks, showcasing their strengths and identifying potential areas for improvement in chatbot technologies.





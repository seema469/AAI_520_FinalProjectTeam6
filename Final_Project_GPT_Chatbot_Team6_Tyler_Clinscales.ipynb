{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT Multi-turn Chatbot**\n",
        "\n",
        "**Chatbot: https://huggingface.co/spaces/tclints/GPT-Multi-Turn-Chatbot**\n",
        "\n",
        "## **Overview**\n",
        "This script utilizes OpenAI's GPT-3.5 API to generate concise answers based on a given context and question. It maintains a conversation history to support multi-turn conversations, with user and assistant messages stored in the conversation_history list. The generate_gpt3_answer function constructs a prompt instructing GPT-3 to provide a concise answer (specifically, a person's name) based on the context and question. The API is called with the prompt, and the response is processed to return only the relevant part of the answer. This ensures that extra details are removed, keeping the output short and focused."
      ],
      "metadata": {
        "id": "8XkU2zUO6rhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "conversation_history = []\n",
        "\n",
        "def add_to_conversation(role, content):\n",
        "    \"\"\"Function to add user/assistant messages to the conversation history.\"\"\"\n",
        "    conversation_history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "def generate_gpt3_answer(context, question):\n",
        "    \"\"\"Generate a concise answer using GPT-3 based on context and question.\"\"\"\n",
        "    prompt_message = (\n",
        "        f\"Answer the following question as concisely as possible, providing only the name of the person:\\n\\nContext: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt_message}\n",
        "        ],\n",
        "        max_tokens=50,\n",
        "        temperature=0.3  # Lower temperature for deterministic responses\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content.strip()\n",
        "\n",
        "    # Post-processing: truncate answer to the first sentence to remove extra details\n",
        "    processed_answer = answer.split('.')[0].strip() + '.'\n",
        "\n",
        "    return processed_answer"
      ],
      "metadata": {
        "id": "EzD1xf-DA73D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, **normalize_answer(s)**, is designed to clean and standardize text for comparison purposes by applying several normalization techniques. Specifically, it performs the following steps:\n",
        "\n",
        "**Convert to Lowercase: **The input text is converted to lowercase to ensure case-insensitive comparisons.\n",
        "\n",
        "**Remove Punctuation:** All punctuation is removed from the text, leaving only words and spaces.\n",
        "\n",
        "**Remove Articles:** Common articles like \"a,\" \"an,\" and \"the\" are removed to avoid unnecessary distinctions.\n",
        "\n",
        "**Fix Whitespace:** Any extra spaces are removed, and the text is reformatted with a single space between words.\n",
        "\n",
        "This normalization process ensures that text is simplified and consistent, making it easier to compare answers in a standardized format."
      ],
      "metadata": {
        "id": "MF6cYutY7W8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punctuation(text):\n",
        "        return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punctuation(lower(s))))"
      ],
      "metadata": {
        "id": "6J0yTfd9uets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**compute_exact_match(prediction, truth):**\n",
        "\n",
        "Purpose: Checks if the predicted answer exactly matches the true answer.\n",
        "How it works:\n",
        "Both the prediction and truth are cleaned using normalize_answer (which removes articles, punctuation, and extra spaces, and makes the text lowercase).\n",
        "If the cleaned prediction is exactly the same as the cleaned truth, it returns 1 (indicating an exact match), otherwise, it returns 0.\n",
        "\n",
        "**compute_f1(prediction, truth):**\n",
        "\n",
        "Purpose: Calculates the F1 Score, which measures how well the predicted answer overlaps with the true answer.\n",
        "How it works:\n",
        "Both the prediction and truth are cleaned and split into individual words (tokens).\n",
        "\n",
        "The score considers how many words overlap between the prediction and truth.\n",
        "It returns 0 if no words match, otherwise it calculates the F1 score based on precision (how many of the predicted words are correct) and recall (how many of the true words were found)."
      ],
      "metadata": {
        "id": "K6HmMlAP70eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_exact_match(prediction, truth):\n",
        "    \"\"\"Computes exact match between the prediction and truth.\"\"\"\n",
        "    return int(normalize_answer(prediction) == normalize_answer(truth))\n",
        "\n",
        "def compute_f1(prediction, truth):\n",
        "    \"\"\"Computes F1 score between the prediction and truth.\"\"\"\n",
        "    pred_tokens = normalize_answer(prediction).split()\n",
        "    truth_tokens = normalize_answer(truth).split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if len(common) == 0:\n",
        "        return 0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall = len(common) / len(truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1"
      ],
      "metadata": {
        "id": "IxVKovrq7Q_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test_gpt3_on_squad** evaluates GPT-3's performance on the SQuAD (Stanford Question Answering Dataset) by comparing its generated answers to the correct answers."
      ],
      "metadata": {
        "id": "JCoQeTYf8HCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_gpt3_on_squad(num_examples=5):\n",
        "    \"\"\"Test GPT-3 on a number of SQuAD examples and compute F1 and Exact Match scores.\"\"\"\n",
        "\n",
        "    # Load the SQuAD dataset\n",
        "    squad_dataset = load_dataset(\"squad\")\n",
        "\n",
        "    total_f1 = 0\n",
        "    total_exact_match = 0\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        example = squad_dataset['train'][i]\n",
        "        context = example['context']\n",
        "        question = example['question']\n",
        "        true_answer = example['answers']['text'][0]\n",
        "\n",
        "        generated_answer = generate_gpt3_answer(context, question)\n",
        "\n",
        "        exact_match = compute_exact_match(generated_answer, true_answer)\n",
        "        f1 = compute_f1(generated_answer, true_answer)\n",
        "\n",
        "        print(f\"Example {i+1}:\")\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Generated Answer: {generated_answer}\")\n",
        "        print(f\"True Answer: {true_answer}\")\n",
        "        print(f\"Exact Match: {exact_match}, F1 Score: {f1}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        total_exact_match += exact_match\n",
        "        total_f1 += f1\n",
        "\n",
        "    avg_f1 = total_f1 / num_examples\n",
        "    avg_exact_match = total_exact_match / num_examples\n",
        "    print(f\"Average Exact Match: {avg_exact_match * 100:.2f}%\")\n",
        "    print(f\"Average F1 Score: {avg_f1 * 100:.2f}%\")\n",
        "\n",
        "# Run the test on the first 30 examples\n",
        "test_gpt3_on_squad(num_examples=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnAESohrsi6V",
        "outputId": "efc22264-01b2-41eb-d5fc-c159c6e77fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "Generated Answer: Saint Bernadette Soubirous.\n",
            "True Answer: Saint Bernadette Soubirous\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 2:\n",
            "Question: What is in front of the Notre Dame Main Building?\n",
            "Generated Answer: Copper statue of Christ.\n",
            "True Answer: a copper statue of Christ\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 3:\n",
            "Question: The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
            "Generated Answer: The Grotto.\n",
            "True Answer: the Main Building\n",
            "Exact Match: 0, F1 Score: 0\n",
            "--------------------------------------------------\n",
            "Example 4:\n",
            "Question: What is the Grotto at Notre Dame?\n",
            "Generated Answer: A Marian place of prayer and reflection.\n",
            "True Answer: a Marian place of prayer and reflection\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 5:\n",
            "Question: What sits on top of the Main Building at Notre Dame?\n",
            "Generated Answer: Golden statue of the Virgin Mary.\n",
            "True Answer: a golden statue of the Virgin Mary\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 6:\n",
            "Question: When did the Scholastic Magazine of Notre dame begin publishing?\n",
            "Generated Answer: September 1876.\n",
            "True Answer: September 1876\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 7:\n",
            "Question: How often is Notre Dame's the Juggler published?\n",
            "Generated Answer: Twice a year.\n",
            "True Answer: twice\n",
            "Exact Match: 0, F1 Score: 0.6666666666666666\n",
            "--------------------------------------------------\n",
            "Example 8:\n",
            "Question: What is the daily student paper at Notre Dame called?\n",
            "Generated Answer: The Observer.\n",
            "True Answer: The Observer\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 9:\n",
            "Question: How many student news papers are found at Notre Dame?\n",
            "Generated Answer: Three.\n",
            "True Answer: three\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 10:\n",
            "Question: In what year did the student paper Common Sense begin publication at Notre Dame?\n",
            "Generated Answer: 1987.\n",
            "True Answer: 1987\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 11:\n",
            "Question: Where is the headquarters of the Congregation of the Holy Cross?\n",
            "Generated Answer: Rome.\n",
            "True Answer: Rome\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 12:\n",
            "Question: What is the primary seminary of the Congregation of the Holy Cross?\n",
            "Generated Answer: Moreau Seminary.\n",
            "True Answer: Moreau Seminary\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 13:\n",
            "Question: What is the oldest structure at Notre Dame?\n",
            "Generated Answer: Old College.\n",
            "True Answer: Old College\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 14:\n",
            "Question: What individuals live at Fatima House at Notre Dame?\n",
            "Generated Answer: Retired priests and brothers.\n",
            "True Answer: Retired priests and brothers\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 15:\n",
            "Question: Which prize did Frederick Buechner create?\n",
            "Generated Answer: Buechner Prize for Preaching.\n",
            "True Answer: Buechner Prize for Preaching\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 16:\n",
            "Question: How many BS level degrees are offered in the College of Engineering at Notre Dame?\n",
            "Generated Answer: Eight.\n",
            "True Answer: eight\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 17:\n",
            "Question: In what year was the College of Engineering at Notre Dame formed?\n",
            "Generated Answer: 1920.\n",
            "True Answer: 1920\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 18:\n",
            "Question: Before the creation of the College of Engineering similar studies were carried out at which Notre Dame college?\n",
            "Generated Answer: College of Science.\n",
            "True Answer: the College of Science\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 19:\n",
            "Question: How many departments are within the Stinson-Remick Hall of Engineering?\n",
            "Generated Answer: Five.\n",
            "True Answer: five\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 20:\n",
            "Question: The College of Science began to offer civil engineering courses beginning at what time at Notre Dame?\n",
            "Generated Answer: 1870s.\n",
            "True Answer: the 1870s\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 21:\n",
            "Question: What entity provides help with the management of time for new students at Notre Dame?\n",
            "Generated Answer: First Year of Studies program.\n",
            "True Answer: Learning Resource Center\n",
            "Exact Match: 0, F1 Score: 0\n",
            "--------------------------------------------------\n",
            "Example 22:\n",
            "Question: How many colleges for undergraduates are at Notre Dame?\n",
            "Generated Answer: Five.\n",
            "True Answer: five\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 23:\n",
            "Question: What was created at Notre Dame in 1962 to assist first year students?\n",
            "Generated Answer: First Year of Studies program.\n",
            "True Answer: The First Year of Studies program\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 24:\n",
            "Question: Which organization declared the First Year of Studies program at Notre Dame \"outstanding?\"\n",
            "Generated Answer: U.\n",
            "True Answer: U.S. News & World Report\n",
            "Exact Match: 0, F1 Score: 0\n",
            "--------------------------------------------------\n",
            "Example 25:\n",
            "Question: The granting of Doctorate degrees first occurred in what year at Notre Dame?\n",
            "Generated Answer: 1924.\n",
            "True Answer: 1924\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 26:\n",
            "Question: What type of degree is an M.Div.?\n",
            "Generated Answer: Master of Divinity.\n",
            "True Answer: Master of Divinity\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 27:\n",
            "Question: Which program at Notre Dame offers a Master of Education degree?\n",
            "Generated Answer: Master of Education program.\n",
            "True Answer: Alliance for Catholic Education\n",
            "Exact Match: 0, F1 Score: 0.25\n",
            "--------------------------------------------------\n",
            "Example 28:\n",
            "Question: In what year was a Master of Arts course first offered at Notre Dame?\n",
            "Generated Answer: 1854.\n",
            "True Answer: 1854\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 29:\n",
            "Question: Which department at Notre Dame is the only one to not offer a PhD program?\n",
            "Generated Answer: The Department of Pre-Professional Studies.\n",
            "True Answer: Department of Pre-Professional Studies\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "--------------------------------------------------\n",
            "Example 30:\n",
            "Question: What institute at Notre Dame studies  the reasons for violent conflict?\n",
            "Generated Answer: The Joan B.\n",
            "True Answer: Joan B. Kroc Institute for International Peace Studies\n",
            "Exact Match: 0, F1 Score: 0.4\n",
            "--------------------------------------------------\n",
            "Average Exact Match: 80.00%\n",
            "Average F1 Score: 84.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Findings**:\n",
        "The GPT-3 model performed well on the SQuAD dataset, achieving an average Exact Match score of 80.00% and an average F1 Score of 84.39% across 30 examples. The model excelled in generating concise, factual answers for straightforward questions but encountered some challenges with more complex or nuanced queries. Overall, the results demonstrate GPT-3's strong capabilities in question-answering tasks. More detailed analysis and insights can be found in the supporting documentation."
      ],
      "metadata": {
        "id": "blZ_Wp6B8VCO"
      }
    }
  ]
}